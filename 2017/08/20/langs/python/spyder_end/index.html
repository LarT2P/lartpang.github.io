<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>关于近期爬虫学习的总结 | 失乐园</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="在之前的三篇文章中，我尝试了使用python爬虫实现的对于特定站点的《剑来》小说的爬取，对于豆瓣的短评的爬取，也有对于爬取的短评数据进行的词云展示，期间运用了不少的知识，现在是时间回顾一下。在此之后，我会再关注一些爬虫框架的使用，以及更多的爬虫的优化方法，争取做到尽量多的吸收新知识，巩固旧知识。 在参考文章爬虫（1）— Python网络爬虫二三事的基础上，我写了这篇文章。 这篇文章主要的目的有两个">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="关于近期爬虫学习的总结">
<meta property="og:url" content="https://plart.pw/2017/08/20/langs/python/spyder_end/index.html">
<meta property="og:site_name" content="失乐园">
<meta property="og:description" content="在之前的三篇文章中，我尝试了使用python爬虫实现的对于特定站点的《剑来》小说的爬取，对于豆瓣的短评的爬取，也有对于爬取的短评数据进行的词云展示，期间运用了不少的知识，现在是时间回顾一下。在此之后，我会再关注一些爬虫框架的使用，以及更多的爬虫的优化方法，争取做到尽量多的吸收新知识，巩固旧知识。 在参考文章爬虫（1）— Python网络爬虫二三事的基础上，我写了这篇文章。 这篇文章主要的目的有两个">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/3471485-097f1343b6952165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/3471485-6a9cfc5e33421cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/3471485-097f1343b6952165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2018-07-13T12:43:37.697Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="关于近期爬虫学习的总结">
<meta name="twitter:description" content="在之前的三篇文章中，我尝试了使用python爬虫实现的对于特定站点的《剑来》小说的爬取，对于豆瓣的短评的爬取，也有对于爬取的短评数据进行的词云展示，期间运用了不少的知识，现在是时间回顾一下。在此之后，我会再关注一些爬虫框架的使用，以及更多的爬虫的优化方法，争取做到尽量多的吸收新知识，巩固旧知识。 在参考文章爬虫（1）— Python网络爬虫二三事的基础上，我写了这篇文章。 这篇文章主要的目的有两个">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/3471485-097f1343b6952165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
    <link rel="alternate" href="/atom.xml" title="失乐园" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">失乐园</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">代码与机器</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://plart.pw"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-langs/python/spyder_end" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/20/langs/python/spyder_end/" class="article-date">
  <time datetime="2017-08-20T13:36:59.000Z" itemprop="datePublished">2017-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/编程与生活/">编程与生活</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      关于近期爬虫学习的总结
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在之前的三篇文章中，我尝试了使用python爬虫实现的对于特定站点的《剑来》小说的爬取，对于豆瓣的短评的爬取，也有对于爬取的短评数据进行的词云展示，期间运用了不少的知识，现在是时间回顾一下。在此之后，我会再关注一些爬虫框架的使用，以及更多的爬虫的优化方法，争取做到尽量多的吸收新知识，巩固旧知识。</p>
<p>在参考文章<a href="http://www.jianshu.com/p/0bfd0c48457f" target="_blank" rel="noopener">爬虫（1）— Python网络爬虫二三事</a>的基础上，我写了这篇文章。</p>
<p><strong>这篇文章主要的目的有两个，收集新知识，并用之丰富旧知识，巩固旧知识。</strong></p>
<h1 id="关于爬虫背后的（这一节是主要是http的概要，下一节是我的一些总结）"><a href="#关于爬虫背后的（这一节是主要是http的概要，下一节是我的一些总结）" class="headerlink" title="关于爬虫背后的（这一节是主要是http的概要，下一节是我的一些总结）"></a>关于爬虫背后的（这一节是主要是http的概要，下一节是我的一些总结）</h1><p>要想不限于代码表面，深入理解爬虫，就得需要了解一些关于网络的知识。</p>
<h2 id="HTTP-协议"><a href="#HTTP-协议" class="headerlink" title="HTTP 协议"></a>HTTP 协议</h2><blockquote>
<p>HTTP（Hypertext Transfer Protocol）是应用级协议，它适应了分布式超媒体协作系统对灵活性及速度的要求。它是一个一般的、无状态的、基于对象的协议。</p>
</blockquote>
<p>可以参考的文章：</p>
<p><strong>想要细致了解协议的可以查看</strong>：</p>
<ul>
<li><a href="http://man.chinaunix.net/develop/rfc/RFC1945.txt" target="_blank" rel="noopener">中文 HTTP/1.0 RFC文档目录</a></li>
<li><a href="http://www.blogjava.net/zjusuyong/articles/304788.html" target="_blank" rel="noopener">深入理解HTTP协议（转）</a></li>
<li><a href="https://www.cnblogs.com/li0803/archive/2008/11/03/1324746.html" target="_blank" rel="noopener">HTTP协议详解（真的很经典）</a></li>
<li><a href="http://tools.jb51.net/table" target="_blank" rel="noopener">HTTP请求方法大全&amp;HTTP请求头大全HTTP状态码大全HTTP Content-type 对照表</a></li>
</ul>
<h3 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h3><ul>
<li>请求（request）：HTTP的请求消息（在第五节定义）</li>
<li>响应（response）：HTTP的回应消息（在第六节定义）</li>
<li>资源（resource）：网络上可以用URI来标识的数据对象或服务。URI有许多名字，如WWW地址、通用文件标识（Universal Document Identifiers）、通用资源标识（Universal Resource Identifiers），以及最终的统一资源定位符（Uniform Resource Locators (URL)）和统一资源名（URN）。</li>
<li>实体（entity）：可被附在请求或回应消息中的特殊的表示法、数据资源的表示、服务资源的回应等，由实体标题（entity header）或实体主体（entity body）内容形式存在的元信息组成。</li>
<li>客户端（client）：指以发出请求为目的而建立连接的应用程序。</li>
<li>用户代理（user agent）：指初始化请求的客户端，如浏览器、编辑器、蜘蛛（web爬行机器人）或其它终端用户工具。用户代理请求标题域包含用户原始请求的信息，这可用于统计方面的用途。通过跟踪协议冲突、自动识别用户代理以避免特殊用户代理的局限性，从而做到更好的回应。虽然没有规定，用户代理应当在请求中包括此域。</li>
<li>服务器（server）：指接受连接，并通过发送回应来响应服务请求的应用程序。</li>
<li>代理（proxy）：同时扮演服务器及客户端角色的中间程序，用来为其它客户产生请求。请求经过变换，被传递到最终的目的服务器，在代理程序内部，请求或被处理，或被传递。代理必须在消息转发前对消息进行解释，而且如有必要还得重写消息。代理通常被用作经过防火墙的客户端出口，用以辅助处理用户代理所没实现的请求。<br><img src="http://upload-images.jianshu.io/upload_images/3471485-097f1343b6952165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></li>
</ul>
<p><em>任何指定的程序都有能力同时做为客户端和服务器。我们在使用这个概念时，不是看程序功能上是否能实现客户及服务器，而是看程序在特定连接时段上扮演何种角色（客户或服务器）。同样，任何服务器可以扮演原始服务器、代理、网关、隧道等角色，行为的切换取决于每次请求的内容。</em></p>
<h3 id="简单流程"><a href="#简单流程" class="headerlink" title="简单流程"></a>简单流程</h3><p>HTTP协议是基于<strong>请求/回应</strong>机制的。客户端与服务器端建立连接后，以请求方法、URI、协议版本等方式向服务器端发出请求，该请求可跟随包含请求修饰符、客户信息、及可能的请求体（body）内容的MIME类型消息。服务器端通过状态队列（status line）来回应，内容包括消息的协议版本、成功或错误代码，也跟随着包含服务器信息、实体元信息及实体内容的MIME类型消息。    </p>
<p>绝大多数HTTP通讯<em>由用户代理进行初始化</em>，并通过它来组装请求以获取存储在一些原始服务器上的资源。</p>
<p>一次HTTP操作称为一个事务，综上，其工作过程可分为四步：</p>
<ol>
<li>首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP的工作开始。</li>
<li>建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。</li>
<li>服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。</li>
<li>客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。</li>
</ol>
<p>如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由HTTP自己完成的，用户只要用鼠标点击，等待信息显示就可以了。</p>
<h3 id="HTTP-消息（HTTP-Message）"><a href="#HTTP-消息（HTTP-Message）" class="headerlink" title="HTTP 消息（HTTP Message）"></a>HTTP 消息（HTTP Message）</h3><p>HTTP消息由客户端到服务器的请求和由服务器到客户端的回应组成。</p>
<pre><code>HTTP-message   = Simple-Request                ; HTTP/0.9 messages
                    | Simple-Response
                    | Full-Request          ; HTTP/1.0 messages
                    | Full-Response
</code></pre><p>完整的请求（Full-Request）和完整的回应（Full-Response）都使用<code>RFC822</code>中实体传输部分规定的消息格式。两者的消息都可能包括标题域（headers，可选）、实体主体（entity body）。实体主体与标题间通过空行来分隔（即CRLF前没有内容的行）。</p>
<pre><code>Full-Request        = Request-Line
                        *( General-Header
                        | Request-Header
                        | Entity-Header )
                        CRLF
                        [ Entity-Body ]

Full-Response          = Status-Line            
                        *( General-Header
                        | Response-Header    
                        | Entity-Header )    
                        CRLF
                        [ Entity-Body ]            
</code></pre><p>（想要了解更多，可以前往<a href="http://man.chinaunix.net/develop/rfc/RFC1945.txt" target="_blank" rel="noopener">中文 HTTP/1.0 RFC文档目录</a> ）</p>
<h1 id="代码实现的流程"><a href="#代码实现的流程" class="headerlink" title="代码实现的流程"></a>代码实现的流程</h1><p>从我之前的代码中可以从一些方法中看出，我们利用<code>urllib</code>库，实际上完成了请求，并接受了响应。通过我们自己构造（例如添加headers）或者使用默认的请求信息，利用了<code>urllib</code>库的<code>request</code>模块的<code>Request()</code>方法构造请求，利用<code>urlopen()</code>方法接受响应返回的页面代码。</p>
<p>至于我们的爬虫工作的过程，在这里盗用(<a href="http://www.jianshu.com/p/0bfd0c48457f）一张图片：" target="_blank" rel="noopener">http://www.jianshu.com/p/0bfd0c48457f）一张图片：</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3471485-6a9cfc5e33421cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="爬虫是一个综合性的工具"><a href="#爬虫是一个综合性的工具" class="headerlink" title="爬虫是一个综合性的工具"></a>爬虫是一个综合性的工具</h1><p>在实现爬虫的过程中，为了实现我们的目的，我们借助了各种各样的工具。浏览器的开发者工具，正则表达式，以及python的各种功能库。可谓是无所不用其极。但是，对于这些工具，我们实际上用到的功能，目前来看，其实并不多，核心代码就是那点而已，所以说，抽离出汇总起来，日后重新使用，一旦对于这个工具不再熟悉，回过头来看这些小小的片段，总是会省下很多的时间。</p>
<p>所以，这篇文章的核心内容就要来了。</p>
<h2 id="获取URL"><a href="#获取URL" class="headerlink" title="获取URL"></a>获取URL</h2><p>这里可以借助浏览器的<code>产看网页源代码</code>，<code>查看元素</code>等开发者工具，且快捷键一般是<code>F12</code>。</p>
<h2 id="获取页面及异常处理"><a href="#获取页面及异常处理" class="headerlink" title="获取页面及异常处理"></a>获取页面及异常处理</h2><p>常用的是利用<code>urllib</code>库。在python3中，没有了原来2中的<code>urllib2</code>，而是用<code>urllib</code>包含了。</p>
<p>主要是用<code>urllib.request.Request()</code>&amp;<code>urllib.request.urlopen()</code></p>
<p><a href="https://docs.python.org/3.0/library/urllib.request.html" target="_blank" rel="noopener">urllib.request</a></p>
<blockquote>
<p>urllib is a package that collects several modules for working with URLs:</p>
</blockquote>
<pre><code>urllib.request  
    for opening and reading URLs
urllib.error 
    containing the exceptions raised by urllib.request
urllib.parse 
    for parsing URLs
urllib.robotparser 
    for parsing robots.txt files
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">url = <span class="string">"..."</span></span><br><span class="line"><span class="keyword">try</span>: </span><br><span class="line">    request = urllib.request.Request(url)</span><br><span class="line">    html = urllib.request.urlopen(request).read().decode(<span class="string">""</span>)<span class="comment"># 按情况解码</span></span><br><span class="line">    print(html) </span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> error_1: </span><br><span class="line">    <span class="keyword">if</span> hasattr(error_1,<span class="string">"code"</span>): </span><br><span class="line">        print(<span class="string">"URLError异常代码："</span>) </span><br><span class="line">        print(error_1.code) </span><br><span class="line">    <span class="keyword">if</span> hasattr(error_1,<span class="string">"reason"</span>): </span><br><span class="line">        print(<span class="string">"URLError异常原因："</span>) </span><br><span class="line">        print(error_1.reason)</span><br><span class="line"><span class="keyword">except</span> urllib.error.HTTPError <span class="keyword">as</span> error_2: </span><br><span class="line">    print(<span class="string">"HTTPError异常概要："</span>)</span><br><span class="line">    print(error_2)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> error_3: </span><br><span class="line">    print(<span class="string">"异常概要："</span>) </span><br><span class="line">    print(error_3) </span><br><span class="line">    print(<span class="string">"---------------------------"</span>) </span><br><span class="line">    errorInfo = sys.exc_info() </span><br><span class="line">    print(<span class="string">"异常类型："</span>+str(errorInfo[<span class="number">0</span>])) </span><br><span class="line">    print(<span class="string">"异常信息或参数："</span>+str(errorInfo[<span class="number">1</span>])) </span><br><span class="line">    print(<span class="string">"调用栈信息的对象："</span>+str(errorInfo[<span class="number">2</span>])) </span><br><span class="line">    print(<span class="string">"已从堆栈中“辗转开解”的函数有关的信息："</span>+str(traceback.print_exc()))</span><br><span class="line"></span><br><span class="line">作者：whenif</span><br><span class="line">链接：http://www.jianshu.com/p/<span class="number">0</span>bfd0c48457f</span><br><span class="line">來源：简书</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
<blockquote>
<p>URLError<br>通常，URLError在没有网络连接(没有路由到特定服务器)，或者服务器不存在的情况下产生。</p>
<p>HTTPError<br>首先我们要明白服务器上每一个HTTP 应答对象response都包含一个数字“状态码”，该状态码表示HTTP协议所返回的响应的状态，这就是HTTPError。比如当产生“404 Not Found”的时候，便表示“没有找到对应页面”，可能是输错了URL地址，也可能IP被该网站屏蔽了，这时便要使用代理IP进行爬取数据。</p>
<p>两者关系</p>
<p>两者是父类与子类的关系，即 <strong>HTTPError是URLError的子类</strong>，HTTPError有异常状态码与异常原因，URLError没有异常状态码。所以，我们在处理的时候，不能使用URLError直接代替HTTPError。同时，Python中所有异常都是基类Exception的成员，所有异常都从此基类继承，而且都在exceptions模块中定义。如果要代替，必须要判断是否有状态码属性。</p>
</blockquote>
<p>异常处理有还有<code>else</code>&amp;<code>finally</code>可以选择。</p>
<h2 id="伪装浏览器"><a href="#伪装浏览器" class="headerlink" title="伪装浏览器"></a>伪装浏览器</h2><p>添加头信息。可以在浏览器的开发者工具中的网路选项卡中点击对应的html页面查看请求报文和相应报文信息。一般的，只需添加用户代理信息即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line"><span class="comment"># 构造方法1</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0'</span>&#125;</span><br><span class="line">request = urllib.request.Request(url, headers = headers)</span><br><span class="line">data = urllib.request.urlopen(request).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造方法2</span></span><br><span class="line">headers=(<span class="string">"User-Agent"</span>, <span class="string">"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"</span>) </span><br><span class="line">opener = urllib.request.build_opener()</span><br><span class="line">opener.addheaders = [headers]</span><br><span class="line"><span class="comment"># # 打开方法1</span></span><br><span class="line">data = opener.open(url, timeout=<span class="number">3</span>).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment"># 打开方法2</span></span><br><span class="line">urllib.request.install_opener(opener)</span><br><span class="line">data = urllib.request.urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url[, data][, headers][, origin_req_host][, unverifiable])</span></span></span><br></pre></td></tr></table></figure>
<p>data 数据可以是指定要发送到服务器的附加数据的字符串，如果不需要这样的数据，则为None。目前HTTP请求是唯一使用数据的请求; 当提供数据参数时，HTTP请求将是POST而不是GET。 数据应该是标准的 <em>application/x-www-form-urlencoded</em> 格式。<code>urllib.parse.urlencode()</code>函数采用映射或2元组的序列，并以此格式返回一个字符串。</p>
<p>headers 应该是一个字典，并且将被视为使用每个键和值作为参数调用 <code>add_header()</code>。 这通常用于“欺骗”用户代理头，浏览器使用它来识别自己 - 一些HTTP服务器只允许来自普通浏览器的请求而不是脚本。</p>
<h2 id="时间问题"><a href="#时间问题" class="headerlink" title="时间问题"></a>时间问题</h2><h3 id="超时重连"><a href="#超时重连" class="headerlink" title="超时重连"></a>超时重连</h3><p>有时候网页请求太过频繁，会出现长时间没有响应的状态，这时候一般需要设置超时重连。下面是一个例子片段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">url = <span class="string">"..."</span></span><br><span class="line">NET_STATUS = <span class="keyword">False</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> NET_STATUS:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = urllib.request.urlopen(url, data=<span class="keyword">None</span>, timeout=<span class="number">3</span>)</span><br><span class="line">        html = response.read().decode(<span class="string">'GBK'</span>)</span><br><span class="line">        print(<span class="string">'NET_STATUS is good'</span>)</span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line">    <span class="keyword">except</span> socket.timeout:</span><br><span class="line">        print(<span class="string">'NET_STATUS is not good'</span>)</span><br><span class="line">        NET_STATUS = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以直接设置全局超时进行捕获，用的还是`socket.timeout`异常</span></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">timeout = <span class="number">3</span></span><br><span class="line">socket.setdefaulttimeout(timeout)</span><br></pre></td></tr></table></figure>
<h3 id="线程延迟"><a href="#线程延迟" class="headerlink" title="线程延迟"></a>线程延迟</h3><p>线程推迟（单位为秒），避免请求太快。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="页面解析"><a href="#页面解析" class="headerlink" title="页面解析"></a>页面解析</h2><p>目前掌握的方法是使用正则表达式和bs库。当然，之后会了解下<code>XPath</code>，这个也提供了一种搜索的思路。</p>
<h3 id="正则匹配"><a href="#正则匹配" class="headerlink" title="正则匹配"></a>正则匹配</h3><p>这个用的函数并不多。有两种书写方式，面向对象和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern = re.compile(<span class="string">'[a-zA-Z]'</span>)</span><br><span class="line">result_list = pattern.findall(<span class="string">'as3SiOPdj#@23awe'</span>)</span><br><span class="line">print(result_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># re.search 扫描整个字符串并返回第一个成功的匹配。 </span></span><br><span class="line">searchObj = re.search( <span class="string">r'(.*) are (.*?) .*'</span>, <span class="string">"Cats are smarter than dogs"</span>, re.M|re.I)</span><br><span class="line">print(searchObj.group())<span class="comment"># Cats are smarter than dogs</span></span><br><span class="line">print(searchObj.group(<span class="number">1</span>))<span class="comment"># Cats</span></span><br><span class="line">print(searchObj.groups())<span class="comment"># ('Cats', 'smarter')</span></span><br></pre></td></tr></table></figure>
<p><a href="www.runoob.com/python3/python3-reg-expressions.html">python3正则表达式</a></p>
<h3 id="beautifulsoup4"><a href="#beautifulsoup4" class="headerlink" title="beautifulsoup4"></a>beautifulsoup4</h3><p><a href="http://www.jb51.net/article/109782.htm" target="_blank" rel="noopener">Python利用Beautiful Soup模块搜索内容详解</a></p>
<p>我个人感觉，这个库的使用，主要难点在于搜索时的麻烦。我觉得比较好用的是方法<code>find()</code>&amp;<code>find_all()</code>&amp;<code>select()</code>。</p>
<p>使用 <code>find()</code> 方法会从搜索结果中返回第一个匹配的内容，而 <code>find_all()</code> 方法则会返回所有匹配的项，返回列表。</p>
<p><code>select()</code>中可以使用CSS选择器。很方便可以参考浏览器开发者工具。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># 可以获取验证码图片地址并下载图片</span></span><br><span class="line">soup = BeautifulSoup(response_login, <span class="string">"html.parser"</span>)</span><br><span class="line">captchaAddr = soup.find(<span class="string">'img'</span>, id=<span class="string">'captcha_image'</span>)[<span class="string">'src'</span>]</span><br><span class="line">request.urlretrieve(captchaAddr, <span class="string">"captcha.jpg"</span>)</span><br><span class="line">...</span><br><span class="line">totalnum = soup.select(<span class="string">"div.mod-hd h2 span a"</span>)[<span class="number">0</span>].get_text()[<span class="number">3</span>:<span class="number">-2</span>]</span><br></pre></td></tr></table></figure>
<h2 id="文件读写"><a href="#文件读写" class="headerlink" title="文件读写"></a>文件读写</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数</span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'w+'</span>) <span class="keyword">as</span> open_file:</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遇到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略</span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'r'</span>, encoding=<span class="string">'gbk'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> open_file:</span><br><span class="line">    all_the_text = open_file.read([size])</span><br><span class="line">    list_of_all_the_lines = open_file.readlines()</span><br><span class="line">    open_file.write(all_the_text)</span><br><span class="line">    open_file.writelines(list_of_text_strings)</span><br></pre></td></tr></table></figure>
<h2 id="登录信息"><a href="#登录信息" class="headerlink" title="登录信息"></a>登录信息</h2><p>我们构造好 POST 请求，这一旦发送过去, 就登陆上了服务器, 服务器就会发给我们 Cookies。继续保持登录状态时，就需要借助相关的库的方式，可以简化处理。</p>
<p>Cookies 是某些网站为了辨别用户身份、进行 session 跟踪而储存在用户本地终端上的数据(通常经过加密)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line">main_url = <span class="string">'...'</span></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="comment"># 看网站post页面需求</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"form_email"</span>:<span class="string">"你的邮箱"</span>,</span><br><span class="line">    <span class="string">"form_password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line">    <span class="string">"source"</span>:<span class="string">"movie"</span>,</span><br><span class="line">    <span class="string">"redir"</span>:<span class="string">"https://movie.douban.com/subject/26934346/"</span>,</span><br><span class="line">    <span class="string">"login"</span>:<span class="string">"登录"</span></span><br><span class="line">&#125;</span><br><span class="line">user_agent = <span class="string">'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agnet'</span>: user_agent, <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>&#125;</span><br><span class="line"></span><br><span class="line">cookie = cookiejar.CookieJar()</span><br><span class="line">cookie_support = request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = request.build_opener(cookie_support)</span><br><span class="line">data = parse.urlencode(formdata).encode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">req_ligin = request.Request(url=main_url, data=data, headers=headers)</span><br><span class="line">html = opener.open(req_ligin).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(html)</span><br></pre></td></tr></table></figure>
<h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><p>有一种反爬虫策略就是对IP进行封锁。所以我们有时需要设置代理。</p>
<p>原理：代理服务器原理如下图，利用代理服务器可以很好处理IP限制问题。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3471485-097f1343b6952165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>一般都是利用互联网上提供的一些免费代理IP进行爬取，而这些免费IP的质量残次不齐，出错是在所难免的，所以在使用之前我们要对其进行有效性测试。另外，对开源IP池有兴趣的同学可以学习Github上的开源项目：<a href="https://github.com/qiyeboy/IPProxyPool" target="_blank" rel="noopener">IPProxyPool</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_proxy</span><span class="params">(url,proxy_addr,iHeaders,timeoutSec)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  功能：伪装成浏览器并使用代理IP防屏蔽</span></span><br><span class="line"><span class="string">  @url：目标URL</span></span><br><span class="line"><span class="string">  @proxy_addr：代理IP地址</span></span><br><span class="line"><span class="string">  @iHeaders：浏览器头信息</span></span><br><span class="line"><span class="string">  @timeoutSec：超时设置（单位：秒）</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  proxy = urllib.request.ProxyHandler(&#123;<span class="string">"http"</span>:proxy_addr&#125;)</span><br><span class="line">  opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)</span><br><span class="line">  urllib.request.install_opener(opener)</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">      req = urllib.request.Request(url,headers = iHeaders)  <span class="comment">#伪装为浏览器并封装request</span></span><br><span class="line">      data = urllib.request.urlopen(req).read().decode(<span class="string">"utf-8"</span>,<span class="string">"ignore"</span>)  </span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> er:</span><br><span class="line">      print(<span class="string">"爬取时发生错误，具体如下："</span>)</span><br><span class="line">      print(er)</span><br><span class="line">  <span class="keyword">return</span> data    </span><br><span class="line">url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">proxy_addr = <span class="string">"125.94.0.253:8080"</span></span><br><span class="line">iHeaders = &#123;<span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0"</span>&#125;</span><br><span class="line">timeoutSec = <span class="number">10</span></span><br><span class="line">data = use_proxy(url,proxy_addr,iHeaders,timeoutSec)</span><br><span class="line">print(len(data))</span><br><span class="line"></span><br><span class="line">作者：whenif</span><br><span class="line">链接：http://www.jianshu.com/p/<span class="number">0</span>bfd0c48457f</span><br><span class="line">來源：简书</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
<h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>后面我会在考虑学习多线程异或多进程改写之前的爬虫，研究研究前面引用的文章里的提到的一些我之前还未了解的技术，感觉要学的东西还是很多，快要开学了，自己的效率有点低，也不知道还能有多少时间给自己这样浪。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://plart.pw/2017/08/20/langs/python/spyder_end/" data-id="cjognmzui0058zchsof54x8g4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/08/24/life/麦兜饭宝奇兵/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          麦兜，幸福的麦兜
        
      </div>
    </a>
  
  
    <a href="/2017/08/18/langs/python/spyderforwordcloud/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">利用豆瓣短评数据生成词云</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/幻想与现实/">幻想与现实</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术与进步/">技术与进步</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/生活与思考/">生活与思考</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/电影与艺术/">电影与艺术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程与生活/">编程与生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/记录与总结/">记录与总结</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-C/">C++&C</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChucK/">ChucK</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css/">css</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/html/">html</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/org-mode/">org-mode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python3/">python3</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/佳片/">佳片</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习/">学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具/">工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开心麻花/">开心麻花</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/影视/">影视</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/总结/">总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/校园/">校园</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/汇编/">汇编</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爱情/">爱情</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/琐事/">琐事</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/生活/">生活</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/电影/">电影</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/科幻/">科幻</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机原理/">计算机原理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机网络/">计算机网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/进阶/">进阶</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/通信/">通信</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随感/">随感</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/C-C/" style="font-size: 14.29px;">C++&C</a> <a href="/tags/ChucK/" style="font-size: 20px;">ChucK</a> <a href="/tags/Java/" style="font-size: 11.43px;">Java</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/css/" style="font-size: 10px;">css</a> <a href="/tags/html/" style="font-size: 10px;">html</a> <a href="/tags/org-mode/" style="font-size: 10px;">org-mode</a> <a href="/tags/python/" style="font-size: 18.57px;">python</a> <a href="/tags/python3/" style="font-size: 10px;">python3</a> <a href="/tags/佳片/" style="font-size: 10px;">佳片</a> <a href="/tags/学习/" style="font-size: 17.14px;">学习</a> <a href="/tags/工具/" style="font-size: 11.43px;">工具</a> <a href="/tags/开心麻花/" style="font-size: 10px;">开心麻花</a> <a href="/tags/影视/" style="font-size: 11.43px;">影视</a> <a href="/tags/总结/" style="font-size: 10px;">总结</a> <a href="/tags/数据结构/" style="font-size: 11.43px;">数据结构</a> <a href="/tags/校园/" style="font-size: 10px;">校园</a> <a href="/tags/汇编/" style="font-size: 15.71px;">汇编</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a> <a href="/tags/爱情/" style="font-size: 10px;">爱情</a> <a href="/tags/琐事/" style="font-size: 10px;">琐事</a> <a href="/tags/生活/" style="font-size: 11.43px;">生活</a> <a href="/tags/电影/" style="font-size: 10px;">电影</a> <a href="/tags/科幻/" style="font-size: 10px;">科幻</a> <a href="/tags/计算机原理/" style="font-size: 12.86px;">计算机原理</a> <a href="/tags/计算机网络/" style="font-size: 10px;">计算机网络</a> <a href="/tags/进阶/" style="font-size: 10px;">进阶</a> <a href="/tags/通信/" style="font-size: 10px;">通信</a> <a href="/tags/随感/" style="font-size: 10px;">随感</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/14/ceshi/">ceshi</a>
          </li>
        
          <li>
            <a href="/2018/10/25/ee/上网背后的流程/">上网背后的流程</a>
          </li>
        
          <li>
            <a href="/2018/09/15/life/这些衰败的日子/">这些衰败的日子</a>
          </li>
        
          <li>
            <a href="/2018/08/30/life/如懿如意/">如懿，如意</a>
          </li>
        
          <li>
            <a href="/2018/08/05/life/开心麻花几部电影/">开心麻花</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Lart Pang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>
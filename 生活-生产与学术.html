<!DOCTYPE HTML>
<!--
    Caminar by TEMPLATED
    templated.co @templatedco
    Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>

<head>
    <title> 给你我的想法 </title>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="tools/assets/css/code.css" rel="stylesheet" />
    <link href="tools/assets/css/templated-caminar.css" rel="stylesheet" />
</head>

<body>

    <!-- Header -->
    <header id="header">
        <div class="logo"><a href="#">给你我的想法 <span>by Lart</span></a></div>
    </header>

    <!-- Main -->
    <section id="main">
        <div class="inner">

            <!-- One 存放文章主体 -->
            <section id="one" class="wrapper style1">

                <div class="image fit flush">
                    <img src="tools/assets/imgs/pic02.jpg" alt="" />
                </div>
                <header class="special">
                    <!-- 存放题目部分 -->
                    <h1 id="_1">生产与学术</h1>
                </header>
                <div class="content">
                    <!-- 用来存放文章内容 -->
                    
<blockquote>
<p>By Lart, 2019-01-08</p>
</blockquote>
<div class="toc">
<ul>
<li><a href="#_1">生产与学术</a><ul>
<li><a href="#_2">这两天</a><ul>
<li><a href="#_3">导出整体模型</a></li>
<li><a href="#caffe2">caffe2模型转换</a></li>
<li><a href="#_4">预处理的补充</a></li>
<li><a href="#_5">安卓的尝试</a><ul>
<li><a href="#aicamera">AiCamera</a></li>
<li><a href="#aicamera-style-transfer">AICamera-Style-Transfer</a></li>
<li><a href="#jejunet">JejuNet</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_6">最后的思考</a><ul>
<li><a href="#_7">没经验</a></li>
<li><a href="#_8">下一步</a></li>
</ul>
</li>
<li><a href="#pickleonnx">pickle和onnx的限制</a></li>
<li><a href="#apk">打包apk安装</a></li>
</ul>
</li>
</ul>
</div>
<p><img alt="upsplash" src="https://images.unsplash.com/photo-1535540878298-a155c6d065ef?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=750&amp;q=80" /></p>
<p>生产与学术, 真实的对立...</p>
<p>这是我这两天对<code>pytorch深度学习-&gt;android实际使用</code>的这个流程的一个切身感受.</p>
<p>说句实在的, 对于模型转换的探索, 算是我这两天最大的收获了...</p>
<p><a href="https://github.com/lartpang/DHSNet-PyTorch/blob/master/converter.ipynb">https://github.com/lartpang/DHSNet-PyTorch/blob/master/converter.ipynb</a></p>
<!--more-->

<h2 id="_2">这两天</h2>
<p>最近在研究将pytorch的模型转换为独立的app, 网上寻找, 找到了一个流程: pytorch-&gt;onnx-&gt;caffe2-&gt;android apk. 主要是基于这篇文章的启发: <a href="https://zhuanlan.zhihu.com/p/32342366">caffe2&amp;pytorch之在移动端部署深度学习模型(全过程!)</a>.</p>
<p>这两天就在折腾这个工具链，为了导出onnx的模型, 不确定要基于怎样的网络, 是已经训练好的, 还是原始搭建网络后再训练来作为基础. 所以不断地翻阅<a href="https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html">pytorch</a>和<a href="https://github.com/onnx/tutorials/tree/master/tutorials">onnx</a>的官方示例, 想要研究出来点什么, 可是, 都是自己手动搭建的模型. 而且使用的是预训练权重, 不是这样:</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">squeezenet1_1</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SqueezeNet 1.1 model from the `official SqueezeNet repo</span>
<span class="sd">    &lt;https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1&gt;`_.</span>
<span class="sd">    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters</span>
<span class="sd">    than SqueezeNet 1.0, without sacrificing accuracy.</span>
<span class="sd">    Args:</span>
<span class="sd">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SqueezeNet</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">model_urls</span><span class="p">[</span><span class="s1">&#39;squeezenet1_1&#39;</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">model</span>
<span class="c1"># Get pretrained squeezenet model</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">squeezenet1_1</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># just a random number</span>
<span class="c1"># Input to the model</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Export the model</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span>
    <span class="n">torch_model</span><span class="p">,</span>        <span class="c1"># model being run</span>
    <span class="n">x</span><span class="p">,</span>                  <span class="c1"># model input (or a tuple for multiple inputs)</span>
    <span class="s2">&quot;squeezenet.onnx&quot;</span><span class="p">,</span>  <span class="c1"># where to save the model (can be a file or file-like object)</span>
    <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># store the trained parameter weights inside the model file</span>
</pre></div>


<p>就是这样:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Create the super-resolution model by using the above model definition.</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">SuperResolutionNet</span><span class="p">(</span><span class="n">upscale_factor</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Load pretrained model weights</span>
<span class="n">model_url</span> <span class="o">=</span> <span class="s1">&#39;https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth&#39;</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># just a random number</span>
<span class="c1"># Initialize model with the pretrained weights</span>
<span class="n">torch_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">model_url</span><span class="p">))</span>
<span class="c1"># set the train mode to false since we will only run the forward pass.</span>
<span class="n">torch_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<p>两种都在载入预训练权重, 直接加载到搭建好的网络上. 对于我手头有的已经训练好的模型, 似乎并不符合这样的条件.</p>
<h3 id="_3">导出整体模型</h3>
<p>最后采用尽可能模仿上面的例子代码的策略, 将整个网络完整的导出(<code>torch.save(model)</code>), 然后再仿照上面那样, 将完整的网络加载(<code>torch.load()</code>)到转换的代码中, 照猫画虎, 以进一步处理.</p>
<blockquote>
<p>这里也很大程度上受到这里的启发: <a href="https://github.com/akirasosa/mobile-semantic-segmentation">https://github.com/akirasosa/mobile-semantic-segmentation</a></p>
</blockquote>
<p>本来想尝试使用之前找到的不论效果还是性能都很强的R3Net进行转换, 可是, 出于作者搭建网络使用的特殊手段, 加上<a href="#pickle和onnx的限制">pickle和onnx的限制</a>, 这个尝试没有奏效, 只好转回头使用之前学习的DHS-Net的代码, 因为它的实现是基于VGG的, 里面的搭建的网络也是需要修改来符合onnx的要求, 主要是更改上采样操作为转置卷积(也就是分数步长卷积, 这里顺带温习了下pytorch里的<code>nn.ConvTranspose2d()</code>的<a href="https://github.com/lartpang/Machine-Deep-Learning/issues/39">计算方式</a>), 因为pytorch的上采样在onnx转换过程中有很多的问题, 特别麻烦, 外加上修改最大池化的一个参数(<code>nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=False)</code>的参数<code>ceil_mode</code>改为<code>ceil_mode=False</code>, 这里参考自前面的知乎专栏的那篇文章), 这样终于可以转换了, 为了方便和快速的测试, 我只是训练了一个epoch, 就直接导出模型, 这次终于可以顺利的<code>torch.save()</code>了.</p>
<div class="codehilite"><pre><span></span><span class="n">filename_opti</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/model-best.pth&#39;</span> <span class="o">%</span> <span class="n">check_root_model</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filename_opti</span><span class="p">)</span>
</pre></div>


<p>之后便利用类似的代码进行了书写.</p>
<div class="codehilite"><pre><span></span><span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>
<span class="n">TMP_ONNX</span> <span class="o">=</span> <span class="s1">&#39;cache/onnx/DHSNet.onnx&#39;</span>
<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s1">&#39;cache/opti/total-opti-current.pth&#39;</span>

<span class="c1"># Convert to ONNX once</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">_export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">TMP_ONNX</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<h3 id="caffe2">caffe2模型转换</h3>
<p>载入模型后, 便可以开始转换了, 这里需要安装caffe2, 官方推荐直接conda安装pytorch1每夜版即可, 会自动安装好依赖.</p>
<blockquote>
<p>说起来这个conda, 就让我又爱又恨, 用它装pytorch从这里可以看出来, 确实不错, 对系统自身的环境没有太多的破坏, 可是用它装tensorflow-gpu的时候, 却是要自动把conda源里的cuda, cudnn工具包都给带上, 有时候似乎会破坏掉系统自身装载的cuda环境(? 不太肯定, 反正现在我不这样装, 直接上pip装, 干净又快速).</p>
</blockquote>
<p>之后的代码中, 主要的问题也就是tensor的cpu/cuda, 或者numpy的转换的问题了. 多尝试一下, 输出下类型就可以看到了.</p>
<div class="codehilite"><pre><span></span><span class="c1"># Let&#39;s also save the init_net and predict_net to a file that we will later use for running them on mobile</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./cache/model_mobile/init_net.pb&#39;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fopen</span><span class="p">:</span>
    <span class="n">fopen</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">init_net</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./cache/model_mobile/predict_net.pb&#39;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fopen</span><span class="p">:</span>
    <span class="n">fopen</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">predict_net</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</pre></div>


<h3 id="_4">预处理的补充</h3>
<p>这里记录下, 查看pytorch的tensor的形状使用<code>tensor.size()</code>方法, 查看numpy数组的形状则使用numpy数组的<code>adarray.shape</code>方法, 而对于PIL(<code>from PIL import Image</code>)读取的Image对象而言, 使用<code>Image.size</code>查看, 而且, 这里只会显示宽和高的长度, 而且Image的对象, 是三维, 在于pytorch的tensor转换的时候, 或者输入网络的时候, 要注意添加维度, 而且要调整通道位置(<code>img = img.transpose(2, 0, 1)</code>).</p>
<p>由于网络保存的部分中, 只涉及到了网络的结构内的部分, 对于数据的预处理的部分并不涉及, 所以说要想真正的利用网络, 还得调整真实的输入, 来作为更适合网络的数据输入.</p>
<p>要注意, 这里针对导出的模型的相关测试, 程实际上是<strong>按照测试网络的流程</strong>来的.</p>
<div class="codehilite"><pre><span></span><span class="c1"># load the resized image and convert it to Ybr format</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;./data/ILSVRC2012_test_00000004_224x224.jpg&quot;</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">img</span> <span class="o">-=</span> <span class="n">mean</span>
<span class="n">img</span> <span class="o">/=</span> <span class="n">std</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>


<h3 id="_5">安卓的尝试</h3>
<p>首先安卓环境的配置就折腾了好久, 一堆破事, 真实的生产开发, 真心不易啊...</p>
<p>这里最终还是失败了, 因为对于安卓的代码是在是不熟悉, 最起码的基础认知都不足, 只有这先前学习Java的一点皮毛知识, 根本不足以二次开发. 也就跑了跑几个完整的demo而已.</p>
<h4 id="aicamera"><a href="https://github.com/caffe2/AICamera">AiCamera</a></h4>
<p>这个跑通了, 但是这是个分类网络的例子, 对于我们要做的分割的任务而言, 有很多细节不一样.</p>
<ul>
<li>输入有差异: 比赛要求的是若是提交apk, 那么要求可以从相册读取图片, 而例子是从摄像头读取的视频数据流. 虽然也处理的是视频帧, 但是要我们再次补充的内容又多了起来, 还是那句话, android一窍不通.</li>
<li>输出有差异: 自我猜测, 比赛为了测评, 输出必然也要输出到相册里, 不然何来测评一说?</li>
</ul>
<h4 id="aicamera-style-transfer"><a href="https://github.com/caffe2/AICamera-Style-Transfer">AICamera-Style-Transfer</a></h4>
<p>这个例子我们参考了一下, 只是因为它的任务是对摄像头视频流数据风格迁移, 而且会直接回显到手机屏幕上, 这里我们主要是想初步实现对于我们网络模型安卓迁移的测试, 在第一个例子的基础上能否实现初步的摄像头视频流的分割, 然后下一步再进一步满足比赛要求.</p>
<p>可是, 尝试失败了. 虽然AS打包成了APK, <a href="#打包apk安装">手机也安装上了</a>, 可是莫名的, 在"loading..."中便闪退了...</p>
<h4 id="jejunet"><a href="https://github.com/tantara/JejuNet">JejuNet</a></h4>
<p>这个例子很给力, 但是使用的是tensorflowlite, 虽然可以用, 能够实现下面的效果, 可是, 不会改.</p>
<p><img alt="img" src="https://raw.githubusercontent.com/tantara/JejuNet/master/docs/20180726-current-results-deeplabv3_on_tf-lite.gif" /></p>
<p>而且是量化网络, 准确率还是有待提升.</p>
<h2 id="_6">最后的思考</h2>
<p>最后还是要思考一下的, 做个总结.</p>
<h3 id="_7">没经验</h3>
<p>吃就吃在没经验的亏上了, 都是初次接触, 之前没怎么接触过安卓, 主要是安卓的开发对于电脑的配置要求太高了, 自己的笔记本根本不够玩的. 也就没有接触过了.</p>
<p>外加上之前的研究学习, 主要是在学术的环境下搞得, 和实际的生产还有很大的距离, 科研与生产的分离, 这对于深度学习这一实际上更偏重实践的领域来说, 有些时候是尤为致命的. 关键时刻下不去手, 这多么无奈, 科学技术无法转化为实实在在的生产力, 忽然有些如梦一般的缥缈.</p>
<p>当然, 最关键的还是, 没有仔细分析赛方的需求, 没有完全思考清楚, 直接就开干了, 这个鲁莽的毛病, 还是没有改掉, 浪费时间不说, 也无助于实际的进度. 赛方的说明含糊, 应该问清楚.</p>
<p>若是担心时间, 那更应该看清楚要求, 切莫随意下手. 比赛说明里只是说要提交一个打包好的应用, 把环境, 依赖什么都处理好, 但是不一定是安卓apk呀, 可以有很多的形式, 但是这也只是最后的一点额外的辅助而已, 重点是模型的性能和效率呢.</p>
<p>莫忘初心, 方得始终. 为什么我想到的是这句.</p>
<h3 id="_8">下一步</h3>
<p>基本上就定了还是使用R3Net, 只能是进一步的细节修改了, 换换后面的循环结构了, 改改连接什么的.</p>
<p>我准备再开始看论文, 学姐的论文可以看看, 似乎提出了一种很不错的后处理的方法, 效果提升很明显, 需要研究下.</p>
<h2 id="pickleonnx">pickle和onnx的限制</h2>
<p>pytorch的<code>torch.save(model)</code>保存模型的时候, 模型架构的代码里<strong>不能使用一些特殊的构建形式</strong>, <a href="https://github.com/zijundeng/R3Net/blob/master/resnext/resnext_101_32x4d_.py">R3Net的ResNeXt结构</a>就用了, 主要是一些lambda结构, 虽然不是太清楚, 但是一般的搭建手段都是可以的.</p>
<p>onnx对于pytorch的支持的操作, 在我的转化中, 主要是最大池化和上采样的问题, 前者可以修改<code>ceil_mode</code>为<code>False</code>, 后者则建议修改为转置卷积, 避免不必要的麻烦. 可见<a href="#导出整体模型">"导出整体模型"</a>小节的描述.</p>
<h2 id="apk">打包apk安装</h2>
<p>这里主要是用release版本构建的apk.</p>
<p>未签名的apk在我的mi 8se(android 8.1)上不能安装, 会解析失败, 需要签名, AS的签名的生成也很简单, 和生成apk在同一级上, 有生成的选项.</p>
                </div>
            </section>

            <!-- Three 放置一些带图的介绍 -->
            <section id="three" class="wrapper">
                <div class="spotlight">
                    <div class="image flush"><img alt="wechat" src="tools/assets/imgs/qrcode_wechat.jpg"
                            title="欢迎关注个人公众号: 码后闲语" /></div>
                    <div class="inner">
                        <h3>码后闲语</h3>
                        <p>这是我的个人公众号, 欢迎有缘人关注</p>
                    </div>
                </div>
            </section>

        </div>
    </section>

    <!-- Footer -->
    <footer id="footer">
        <div class="copyright">
            <p><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
                    <img alt="知识共享许可协议" style="border-width:0"
                        src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />
                本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0
                    国际许可协议</a>进行许可.</p>

            &copy; Untitled. All rights reserved. Images <a href="https://unsplash.com">Unsplash</a> Design <a
                href="https://templated.co">TEMPLATED</a>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="tools/assets/js/jquery.min.js"></script>
    <script src="tools/assets/js/jquery.poptrox.min.js"></script>
    <script src="tools/assets/js/skel.min.js"></script>
    <script src="tools/assets/js/util.js"></script>
    <script src="tools/assets/js/main.js"></script>

</body>

</html>

---
date: 2019-01-08
title: "生产与学术"
tags: [
    "总结",
    "编程",
    "pytorch",
    "onnx",
    "安卓",
]
categories: [
    "生活"
]
---

# 生产与学术

> By Lart, 2019-01-08

![upsplash](https://images.unsplash.com/photo-1535540878298-a155c6d065ef?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=750&q=80)

生产与学术, 真实的对立...

这是我这两天对`pytorch深度学习->android实际使用`的这个流程的一个切身感受.

说句实在的, 对于模型转换的探索, 算是我这两天最大的收获了...

<https://github.com/lartpang/DHSNet-PyTorch/blob/master/converter.ipynb>

<!--more-->

## 这两天

最近在研究将pytorch的模型转换为独立的app, 网上寻找, 找到了一个流程: pytorch->onnx->caffe2->android apk. 主要是基于这篇文章的启发: [caffe2&pytorch之在移动端部署深度学习模型(全过程!)](https://zhuanlan.zhihu.com/p/32342366).

这两天就在折腾这个工具链，为了导出onnx的模型, 不确定要基于怎样的网络, 是已经训练好的, 还是原始搭建网络后再训练来作为基础. 所以不断地翻阅[pytorch](https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html)和[onnx](https://github.com/onnx/tutorials/tree/master/tutorials)的官方示例, 想要研究出来点什么, 可是, 都是自己手动搭建的模型. 而且使用的是预训练权重, 不是这样:

```python
def squeezenet1_1(pretrained=False, **kwargs):
    r"""SqueezeNet 1.1 model from the `official SqueezeNet repo
    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.
    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters
    than SqueezeNet 1.0, without sacrificing accuracy.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = SqueezeNet(version=1.1, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_1']))
    return model
# Get pretrained squeezenet model
torch_model = squeezenet1_1(True)

from torch.autograd import Variable
batch_size = 1    # just a random number
# Input to the model
x = Variable(torch.randn(batch_size, 3, 224, 224), requires_grad=True)
# Export the model
torch_out = torch.onnx._export(
    torch_model,        # model being run
    x,                  # model input (or a tuple for multiple inputs)
    "squeezenet.onnx", 	# where to save the model (can be a file or file-like object)
    export_params=True) # store the trained parameter weights inside the model file
```

就是这样:

```python
# Create the super-resolution model by using the above model definition.
torch_model = SuperResolutionNet(upscale_factor=3)
# Load pretrained model weights
model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'
batch_size = 1    # just a random number
# Initialize model with the pretrained weights
torch_model.load_state_dict(model_zoo.load_url(model_url))
# set the train mode to false since we will only run the forward pass.
torch_model.train(False)
```

两种都在载入预训练权重, 直接加载到搭建好的网络上. 对于我手头有的已经训练好的模型, 似乎并不符合这样的条件.

### 导出整体模型

最后采用尽可能模仿上面的例子代码的策略, 将整个网络完整的导出(`torch.save(model)`), 然后再仿照上面那样, 将完整的网络加载(`torch.load()`)到转换的代码中, 照猫画虎, 以进一步处理.

> 这里也很大程度上受到这里的启发: <https://github.com/akirasosa/mobile-semantic-segmentation>

本来想尝试使用之前找到的不论效果还是性能都很强的R3Net进行转换, 可是, 出于作者搭建网络使用的特殊手段, 加上[pickle和onnx的限制](#pickle和onnx的限制), 这个尝试没有奏效, 只好转回头使用之前学习的DHS-Net的代码, 因为它的实现是基于VGG的, 里面的搭建的网络也是需要修改来符合onnx的要求, 主要是更改上采样操作为转置卷积(也就是分数步长卷积, 这里顺带温习了下pytorch里的`nn.ConvTranspose2d()`的[计算方式](https://github.com/lartpang/Machine-Deep-Learning/issues/39)), 因为pytorch的上采样在onnx转换过程中有很多的问题, 特别麻烦, 外加上修改最大池化的一个参数(`nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=False)`的参数`ceil_mode`改为`ceil_mode=False`, 这里参考自前面的知乎专栏的那篇文章), 这样终于可以转换了, 为了方便和快速的测试, 我只是训练了一个epoch, 就直接导出模型, 这次终于可以顺利的`torch.save()`了.

```python
filename_opti = ('%s/model-best.pth' % check_root_model)
torch.save(model, filename_opti)
```

之后便利用类似的代码进行了书写.

```python
IMG_SIZE = 224
TMP_ONNX = 'cache/onnx/DHSNet.onnx'
MODEL_PATH = 'cache/opti/total-opti-current.pth'

# Convert to ONNX once
model = torch.load(MODEL_PATH).cuda()
model.train(False)

x = Variable(torch.randn(1, 3, 224, 224), requires_grad=True).cuda()
torch_out = torch.onnx._export(model, x, TMP_ONNX, export_params=True)
```

### caffe2模型转换

载入模型后, 便可以开始转换了, 这里需要安装caffe2, 官方推荐直接conda安装pytorch1每夜版即可, 会自动安装好依赖.

> 说起来这个conda, 就让我又爱又恨, 用它装pytorch从这里可以看出来, 确实不错, 对系统自身的环境没有太多的破坏, 可是用它装tensorflow-gpu的时候, 却是要自动把conda源里的cuda, cudnn工具包都给带上, 有时候似乎会破坏掉系统自身装载的cuda环境(? 不太肯定, 反正现在我不这样装, 直接上pip装, 干净又快速).

之后的代码中, 主要的问题也就是tensor的cpu/cuda, 或者numpy的转换的问题了. 多尝试一下, 输出下类型就可以看到了.

```python
# Let's also save the init_net and predict_net to a file that we will later use for running them on mobile
with open('./cache/model_mobile/init_net.pb', "wb") as fopen:
    fopen.write(init_net.SerializeToString())
with open('./cache/model_mobile/predict_net.pb', "wb") as fopen:
    fopen.write(predict_net.SerializeToString())
```

### 预处理的补充

这里记录下, 查看pytorch的tensor的形状使用`tensor.size()`方法, 查看numpy数组的形状则使用numpy数组的`adarray.shape`方法, 而对于PIL(`from PIL import Image`)读取的Image对象而言, 使用`Image.size`查看, 而且, 这里只会显示宽和高的长度, 而且Image的对象, 是三维, 在于pytorch的tensor转换的时候, 或者输入网络的时候, 要注意添加维度, 而且要调整通道位置(`img = img.transpose(2, 0, 1)`).

由于网络保存的部分中, 只涉及到了网络的结构内的部分, 对于数据的预处理的部分并不涉及, 所以说要想真正的利用网络, 还得调整真实的输入, 来作为更适合网络的数据输入.

要注意, 这里针对导出的模型的相关测试, 程实际上是**按照测试网络的流程**来的.

```python
# load the resized image and convert it to Ybr format
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])
img = Image.open("./data/ILSVRC2012_test_00000004_224x224.jpg")
img = np.array(img)
img = img.astype(np.float64) / 255
img -= mean
img /= std
img = img.transpose(2, 0, 1)
```

### 安卓的尝试

首先安卓环境的配置就折腾了好久, 一堆破事, 真实的生产开发, 真心不易啊...

这里最终还是失败了, 因为对于安卓的代码是在是不熟悉, 最起码的基础认知都不足, 只有这先前学习Java的一点皮毛知识, 根本不足以二次开发. 也就跑了跑几个完整的demo而已.

####  [AiCamera](https://github.com/caffe2/AICamera)

这个跑通了, 但是这是个分类网络的例子, 对于我们要做的分割的任务而言, 有很多细节不一样.

* 输入有差异: 比赛要求的是若是提交apk, 那么要求可以从相册读取图片, 而例子是从摄像头读取的视频数据流. 虽然也处理的是视频帧, 但是要我们再次补充的内容又多了起来, 还是那句话, android一窍不通.
* 输出有差异: 自我猜测, 比赛为了测评, 输出必然也要输出到相册里, 不然何来测评一说?

#### [AICamera-Style-Transfer](https://github.com/caffe2/AICamera-Style-Transfer)

这个例子我们参考了一下, 只是因为它的任务是对摄像头视频流数据风格迁移, 而且会直接回显到手机屏幕上, 这里我们主要是想初步实现对于我们网络模型安卓迁移的测试, 在第一个例子的基础上能否实现初步的摄像头视频流的分割, 然后下一步再进一步满足比赛要求.

可是, 尝试失败了. 虽然AS打包成了APK, [手机也安装上了](#打包apk安装), 可是莫名的, 在"loading..."中便闪退了...

#### [JejuNet](https://github.com/tantara/JejuNet)

这个例子很给力, 但是使用的是tensorflowlite, 虽然可以用, 能够实现下面的效果, 可是, 不会改.

![img](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/20180726-current-results-deeplabv3_on_tf-lite.gif)

而且是量化网络, 准确率还是有待提升.

## 最后的思考

最后还是要思考一下的, 做个总结.

### 没经验

吃就吃在没经验的亏上了, 都是初次接触, 之前没怎么接触过安卓, 主要是安卓的开发对于电脑的配置要求太高了, 自己的笔记本根本不够玩的. 也就没有接触过了.

外加上之前的研究学习, 主要是在学术的环境下搞得, 和实际的生产还有很大的距离, 科研与生产的分离, 这对于深度学习这一实际上更偏重实践的领域来说, 有些时候是尤为致命的. 关键时刻下不去手, 这多么无奈, 科学技术无法转化为实实在在的生产力, 忽然有些如梦一般的缥缈.

当然, 最关键的还是, 没有仔细分析赛方的需求, 没有完全思考清楚, 直接就开干了, 这个鲁莽的毛病, 还是没有改掉, 浪费时间不说, 也无助于实际的进度. 赛方的说明含糊, 应该问清楚.

若是担心时间, 那更应该看清楚要求, 切莫随意下手. 比赛说明里只是说要提交一个打包好的应用, 把环境, 依赖什么都处理好, 但是不一定是安卓apk呀, 可以有很多的形式, 但是这也只是最后的一点额外的辅助而已, 重点是模型的性能和效率呢.

莫忘初心, 方得始终. 为什么我想到的是这句.

### 下一步

基本上就定了还是使用R3Net, 只能是进一步的细节修改了, 换换后面的循环结构了, 改改连接什么的.

我准备再开始看论文, 学姐的论文可以看看, 似乎提出了一种很不错的后处理的方法, 效果提升很明显, 需要研究下.

## pickle和onnx的限制

pytorch的`torch.save(model)`保存模型的时候, 模型架构的代码里**不能使用一些特殊的构建形式**, [R3Net的ResNeXt结构](https://github.com/zijundeng/R3Net/blob/master/resnext/resnext_101_32x4d_.py)就用了, 主要是一些lambda结构, 虽然不是太清楚, 但是一般的搭建手段都是可以的.

onnx对于pytorch的支持的操作, 在我的转化中, 主要是最大池化和上采样的问题, 前者可以修改`ceil_mode`为`False`, 后者则建议修改为转置卷积, 避免不必要的麻烦. 可见["导出整体模型"](#导出整体模型)小节的描述.

## 打包apk安装

这里主要是用release版本构建的apk.

未签名的apk在我的mi 8se(android 8.1)上不能安装, 会解析失败, 需要签名, AS的签名的生成也很简单, 和生成apk在同一级上, 有生成的选项.
